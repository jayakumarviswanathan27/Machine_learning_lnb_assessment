# -*- coding: utf-8 -*-
"""Python_ML_14_09_2023.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ebpKHqR1I0Rv55wx2cJzEk-DpgLZqL0h
"""



"""**PROGRAMMING**

**1.Binary Classification with scikit-learn**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

data=pd.DataFrame({
    "Age":[25,30,35,40,45,50,55,60,65,70],
    "Income":[40000,45000,60000,80000,90000,75000,55000,60000,50000,45000],
    "Buy":[1,0,1,1,0,1,0,0,0,1]

})

X = data.drop(columns=['Buy'])
y = data['Buy']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)


print(f"Accuracy: {accuracy}")

from sklearn.metrics import confusion_matrix
conf_matrix = confusion_matrix(y_test, y_pred)

conf_matrix

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
sns.heatmap(conf_matrix, annot=True, fmt="d")
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
print(classification_report(y_test, y_pred))

"""**2.multiclass classification model using scikit-learn**"""



from sklearn.tree import DecisionTreeClassifier

data = pd.DataFrame({
"weight (grams)":[120, 150, 130, 140, 180, 200, 160, 190, 170, 110],
"color (1=Red,2=Yellow, 3=Orange)": [1, 2, 1, 2, 3, 2, 3, 1, 2, 3],
"Fruit":["Apple","Banana", "Apple", "Banana", 'Orange', "Banana", "Orange", "Apple", "Banana", "Apple"]
})

X = data.drop(columns=['Fruit'])
y = data['Fruit']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = DecisionTreeClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

acc = accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc}")

from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
sns.heatmap(conf_matrix, annot=True, fmt="d")
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
print(classification_report(y_test, y_pred))

"""**Question 3: Clustering with scikit-learn**"""



from sklearn.cluster import KMeans
# Create a dummy dataset
data = pd.DataFrame({
    'Age': [22, 24, 30, 20, 25, 28, 35, 40, 50, 60],
'Income (in $1000)': [20, 22, 25, 18, 35, 40, 60, 55, 80, 85]
})

kmeans = KMeans(n_clusters=4, random_state=42)
data['Cluster'] = kmeans.fit_predict(data[['Age', 'Income (in $1000)']])

plt.figure(figsize=(6, 6))
plt.scatter(data['Age'], data['Income (in $1000)'], c=data['Cluster'], cmap='viridis')
plt.title('K-Means Clustering')
plt.xlabel('Age')
plt.ylabel('Income (in $1000)')
plt.show()



"""**Question 4: Regression with scikit-learn**"""

from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
data = pd.DataFrame({
    'Bedrooms': [2, 3, 2, 4, 3, 3, 4, 2, 5, 4],
'Square Footage': [1200, 1500, 1300, 2000, 1800, 1600, 2200, 1100, 2500, 2100],
'Price (in $1000)': [150, 200, 180, 250, 220, 210, 280, 140, 320, 290]
})
# TODO: Split the data into training and testing sets
# TODO: Build a regression model (e.g., Linear Regression)
# TODO: Train the model on the training data
# TODO: Make predictions on the test data
# TODO: Evaluate the model's performance using mean squared error and R-squared

X = data[['Bedrooms', 'Square Footage']]
y = data['Price (in $1000)']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R2): {r2}")





"""**Question 5: Natural Language Processing (NLP) with Text Classification**"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

data= pd.DataFrame({
    'Text':["This movie is amazing!","I didn't like it at all.","Great plot and acting.","Worst movie ever!", "Enjoyed every moment.", "Terrible screenplay.","Highly recommended!", "Couldn't finish it.", "Loved it!"],
'Sentiment': ['Positive', "Negative", 'Positive', "Negative", "Positive", 'Negative', 'Positive', 'Negative', 'Positive']
})

X = data['Text']
y = data['Sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

tfidf_vectorizer = TfidfVectorizer()
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

naive_bayes = MultinomialNB()
naive_bayes.fit(X_train_tfidf, y_train)

y_pred = naive_bayes.predict(X_test_tfidf)
print(y_pred,y_test)

accuracy = accuracy_score(y_test, y_pred)
classification_report_output = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("\nClassification Report:\n", classification_report_output)





"""**PANDAS PROGRAMING**

**question 1**
"""

import pandas as pd
data = {
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 22]
}

df = pd.DataFrame(data)
print(df)

alice_age = df[df['Name'] == 'Alice']['Age'].values[0]
print(f"The age of Alice is: {alice_age}")
average_age = df['Age'].mean()
print("The average age of all individuals is:",average_age)

"""**question 2**"""

import pandas as pd
data = {
    'City': ["New York", "Los Angeles", "Chicago"],
    'Population': [8623000, 3999759, 2716000]
}

df = pd.DataFrame(data)
print(df)

largest_population_city = df[df['Population'] == df['Population'].max()]['City'].values[0]
print("The city with the largest population is:",largest_population_city)

total_population = df['Population'].sum()
print("The total population of all cities is:",total_population)

"""**question 3**"""

import numpy as np
arr = np.random.rand(20)
print(arr)
mean_value = np.mean(arr)
print(f"The mean of arr is: {mean_value:.2f}")
count_greater_than_0_5 = np.sum(arr > 0.5)
print(f"The number of values in 'arr' greater than 0.5 is: {count_greater_than_0_5}")

"""**question 6**"""

import numpy as np
arr = np.random.rand(25)
print(arr)
median_value = np.median(arr)
print(f"The median value of arr is: {median_value:.2f}")
count_less_than_0_3 = np.sum(arr < 0.3)
print(f"The number of values in 'arr' less than 0.3 is: {count_less_than_0_3}")



"""**Question 5**"""

import pandas as pd
data = {'City': ['London', 'Paris', 'Berlin'],
        'Population': [8787892, 2206488, 3644826],
        'Area (in square miles)': [607, 40.7, 344]}
df = pd.DataFrame(data)
print(df)

df['Population Density'] = df['Population'] / df['Area (in square miles)']
highest_population_density_city = df[df['Population Density'] == df['Population Density'].max()]['City'].values[0]
print("City with Highest Population Density:", highest_population_density_city)

total_area = df['Area (in square miles)'].sum()
print("Total Area:",total_area)



"""**MCQ Machine learning**"""

1) B. 12

2) C. It cannot be determined from the given information.

3) A. Cluster 0

4) C. Time series prediction

5) B. It retains the top 10 principal components.

6) C. The cumulative reward obtained from 100 episodes.





"""**MCQ NUMPY AND PANDAS**"""

1) A [5, 7, 9]

2) B [2, 4]

3) ["Bob"]

4) 4.41

5) A. [10, 40, 90, 160, 250]

6) A. [5, 7, 9]

7) ["Bob"]

8) B. [3, 1, 4, 5, 9, 2, 6]

9) B. [6, 15]

10) C.4.0